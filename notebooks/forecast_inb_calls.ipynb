{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forecasting Inbound Calls by Hour\n",
        "\n",
        "This notebook implements a forecasting model for inbound calls using LSTM neural networks. It includes data preprocessing, anomaly detection, feature engineering, model training, evaluation, and future predictions. Additionally, it calculates required operators using ErlangC for workforce management.\n",
        "\n",
        "Author: [Your Name]\n",
        "Date: October 23, 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Libraries\n",
        "\n",
        "Import necessary libraries for data manipulation, visualization, machine learning, and time series analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 5)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from warnings import catch_warnings, filterwarnings\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Neural network libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# For anomaly detection\n",
        "from sklearn.ensemble import IsolationForest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Import and Preprocessing\n",
        "\n",
        "Load the dataset and perform initial transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('tmp001 05112024.csv', sep=';', header=0, parse_dates=True)\n",
        "\n",
        "# Replace commas with dots and convert to float\n",
        "data[['IS_LUNCH', 'IS_WORKTIME', 'IS_MORNING']] = data[['IS_LUNCH', 'IS_WORKTIME', 'IS_MORNING']].replace(',', '.', regex=True).astype(float)\n",
        "\n",
        "# Print data types to verify\n",
        "print(data[['IS_LUNCH', 'IS_WORKTIME', 'IS_MORNING']].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert columns to appropriate formats\n",
        "data['MONTH'] = pd.to_datetime(data['MONTH'] + '-01')  # Convert month to datetime\n",
        "data['DATESTART'] = pd.to_datetime(data['DATESTART'], dayfirst=True)  # Convert date to datetime\n",
        "data['RAZREZ'] = pd.to_timedelta(data['RAZREZ'] + ':00')  # Convert time to timedelta\n",
        "\n",
        "# Create full timestamp\n",
        "data['TIMESTAMP'] = data['DATESTART'] + data['RAZREZ']\n",
        "data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter records starting from 2022-04-01\n",
        "Hourly_calls = data[(data['DATESTART'] >= pd.to_datetime('2022-04-01'))]\n",
        "Hourly_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set TIMESTAMP as index\n",
        "Hourly_calls.set_index('TIMESTAMP', inplace=True)\n",
        "Hourly_calls.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomaly Detection and Correction\n",
        "\n",
        "Detect and handle anomalies in the call volume data using Isolation Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for anomaly detection\n",
        "anomaly_analysis = Hourly_calls.copy()\n",
        "\n",
        "# Plot original call volume\n",
        "anomaly_analysis['CNT_CALLS'].plot(figsize=(12, 6))\n",
        "plt.title('Call Volume')\n",
        "plt.show()\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(anomaly_analysis[['CNT_CALLS']])\n",
        "\n",
        "# Train Isolation Forest model\n",
        "model = IsolationForest(contamination=0.001)\n",
        "model.fit(scaled_data)\n",
        "\n",
        "# Predict anomalies\n",
        "anomaly_analysis['anomaly'] = model.predict(scaled_data)\n",
        "anomaly_analysis['anomaly'] = anomaly_analysis['anomaly'].map({1: 'normal', -1: 'anomaly'})\n",
        "\n",
        "# Visualize anomalies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(anomaly_analysis.index, anomaly_analysis['CNT_CALLS'], label='Number of calls')\n",
        "plt.scatter(anomaly_analysis.index[anomaly_analysis['anomaly'] == 'anomaly'], \n",
        "            anomaly_analysis['CNT_CALLS'][anomaly_analysis['anomaly'] == 'anomaly'], \n",
        "            color='red', label='Anomalies')\n",
        "plt.legend()\n",
        "plt.title('Anomalies in Call Volume Data')\n",
        "plt.show()\n",
        "\n",
        "# Replace anomalies with median\n",
        "median_value = anomaly_analysis['CNT_CALLS'].median()\n",
        "anomaly_analysis.loc[anomaly_analysis['anomaly'] == 'anomaly', 'CNT_CALLS'] = median_value\n",
        "\n",
        "# Plot corrected data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(anomaly_analysis.index, anomaly_analysis['CNT_CALLS'], label='Corrected Call Volume')\n",
        "plt.title('Corrected Call Volume Data')\n",
        "plt.show()\n",
        "\n",
        "Hourly_calls = anomaly_analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Create new features such as weekday dummies, time interval indicators, billing days, lag features, and rolling statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add weekday feature (Monday=0, ..., Sunday=6)\n",
        "Hourly_calls['weekday'] = Hourly_calls.index.weekday\n",
        "\n",
        "# One-hot encode weekday\n",
        "weekday_dummies = pd.get_dummies(Hourly_calls['weekday'], prefix='weekday')\n",
        "weekday_dummies = weekday_dummies.astype(int)\n",
        "Hourly_calls = Hourly_calls.join(weekday_dummies, how='left')\n",
        "Hourly_calls.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dummy variables for unique time intervals\n",
        "Hourly_calls['RAZREZ'] = pd.to_timedelta(Hourly_calls['RAZREZ'])\n",
        "unique_razrez = Hourly_calls['RAZREZ'].unique()\n",
        "\n",
        "for razrez in unique_razrez:\n",
        "    col_name = f'IS_{str(razrez).replace(\" \", \"_\").replace(\"days\", \"d\").replace(\":\", \"_\")}'\n",
        "    Hourly_calls[col_name] = Hourly_calls['RAZREZ'].apply(lambda x: 1 if x == razrez else 0)\n",
        "\n",
        "Hourly_calls.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions to identify critical and billing days\n",
        "def is_critical_day(date):\n",
        "    \"\"\"Check if the day is a critical day (pre-billing).\"\"\"\n",
        "    return 1 if date.day in [3, 4, 10, 13, 14, 23, 24] else 0\n",
        "\n",
        "def is_billing_day(date):\n",
        "    \"\"\"Check if the day is a billing day.\"\"\"\n",
        "    return 1 if date.day in [5, 15, 25] else 0\n",
        "\n",
        "# Add critical_day and billing_day features\n",
        "Hourly_calls['critical_day'] = Hourly_calls['DATESTART'].apply(is_critical_day)\n",
        "Hourly_calls['billing_day'] = Hourly_calls['DATESTART'].apply(is_billing_day)\n",
        "Hourly_calls.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add lag features (previous 10 periods)\n",
        "for i in range(1, 11):\n",
        "    Hourly_calls[f'Inq_{i}'] = Hourly_calls['CNT_CALLS'].shift(i)\n",
        "\n",
        "Hourly_calls.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate rolling mean (7 periods) and std (3 periods)\n",
        "Hourly_calls['Inq_mean_7'] = Hourly_calls['Inq_1'].rolling(window=7).mean()\n",
        "Hourly_calls['Inq_std_3'] = Hourly_calls['Inq_1'].rolling(window=3).std()\n",
        "Hourly_calls.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation for Modeling\n",
        "\n",
        "Prepare features and target for the LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop unnecessary columns and fill NaNs\n",
        "final_df = Hourly_calls.drop(columns=[\n",
        "    'weekday', 'DATESTART', 'MONTH', 'RAZREZ', 'anomaly',\n",
        "    'IS_0_d_22_00_00', 'IS_0_d_00_00_00', 'IS_0_d_00_30_00',\n",
        "    'IS_0_d_01_00_00', 'IS_0_d_01_30_00', 'IS_0_d_02_30_00',\n",
        "    'IS_0_d_03_00_00', 'IS_0_d_04_00_00', 'IS_0_d_05_30_00',\n",
        "    'IS_0_d_06_00_00', 'IS_0_d_06_30_00', 'IS_0_d_07_00_00',\n",
        "    'IS_0_d_07_30_00', 'IS_0_d_22_30_00', 'IS_0_d_23_00_00',\n",
        "    'IS_0_d_23_30_00', 'IS_0_d_04_30_00', 'IS_0_d_03_30_00',\n",
        "    'IS_0_d_02_00_00', 'IS_0_d_05_00_00'\n",
        "]).fillna(0)\n",
        "\n",
        "final_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate target and features\n",
        "y = final_df['CNT_CALLS']\n",
        "X = final_df.drop(columns=['CNT_CALLS'])\n",
        "\n",
        "# Scale numerical features\n",
        "scaler_x = StandardScaler()\n",
        "numerical_columns = ['Inq_1', 'Inq_2', 'Inq_3', 'Inq_4', 'Inq_5', 'Inq_6',\n",
        "                     'Inq_7', 'Inq_8', 'Inq_9', 'Inq_10', 'Inq_mean_7', 'Inq_std_3']\n",
        "X[numerical_columns] = scaler_x.fit_transform(X[numerical_columns])\n",
        "\n",
        "X_new = X.values\n",
        "\n",
        "# Scale target\n",
        "scaler_y = StandardScaler()\n",
        "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for LSTM\n",
        "time_steps = 1\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], time_steps, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], time_steps, X_test.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM Model Definition and Training\n",
        "\n",
        "Define and train an LSTM model for time series forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LSTM model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(units=300, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=300))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile model\n",
        "learning_rate = 0.00001\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['mae'], label='MAE (Train)')\n",
        "plt.plot(history.history['val_mae'], label='MAE (Validation)')\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Loss (Train)')\n",
        "plt.plot(history.history['val_loss'], label='Loss (Validation)')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluate the model on train and test sets using MAE, RMSE, and R²."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inverse scale predictions and targets\n",
        "y_train_original = scaler_y.inverse_transform(y_train)\n",
        "y_test_original = scaler_y.inverse_transform(y_test)\n",
        "\n",
        "# Train predictions\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_train_pred_original = scaler_y.inverse_transform(y_train_pred)\n",
        "\n",
        "# Test predictions\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred_original = scaler_y.inverse_transform(y_test_pred)\n",
        "\n",
        "# Calculate metrics\n",
        "train_mae = mean_absolute_error(y_train_original, y_train_pred_original)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train_original, y_train_pred_original))\n",
        "train_r2 = r2_score(y_train_original, y_train_pred_original)\n",
        "print(\"Train MAE: \", train_mae)\n",
        "print('Train RMSE:', train_rmse)\n",
        "print(\"Train R²:\", train_r2)\n",
        "\n",
        "test_mae = mean_absolute_error(y_test_original, y_test_pred_original)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test_original, y_test_pred_original))\n",
        "test_r2 = r2_score(y_test_original, y_test_pred_original)\n",
        "print(\"Test MAE: \", test_mae)\n",
        "print('Test RMSE:', test_rmse)\n",
        "print(\"Test R²:\", test_r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare DataFrames for visualization\n",
        "train_rf = pd.DataFrame({'train': y_train_original.ravel(), 'train_pred': y_train_pred_original.ravel()})\n",
        "predictions_rf = pd.DataFrame({'y_test': y_test_original.ravel(), 'y_test_pred': y_test_pred_original.ravel()})\n",
        "\n",
        "# Plot test predictions\n",
        "plt.figure(figsize=(20, 6))\n",
        "plt.plot(predictions_rf[['y_test', 'y_test_pred']].iloc[4:260].sort_index(), label=['Test', 'Predictions'])\n",
        "plt.legend(['Test', 'Predictions'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Residuals plot\n",
        "predictions_rf['residuals'] = predictions_rf['y_test'] - predictions_rf['y_test_pred']\n",
        "sns.scatterplot(data=predictions_rf, x=\"y_test_pred\", y=\"residuals\")\n",
        "plt.axhline(y=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Future Predictions\n",
        "\n",
        "Generate features for future timestamps and predict call volumes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate future time range\n",
        "start_time = (final_df.index[-1] + pd.Timedelta(minutes=30)).to_pydatetime()\n",
        "end_time = start_time + pd.Timedelta(days=35)\n",
        "time_range = pd.date_range(start=start_time, end=end_time, freq='30T')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_features(df):\n",
        "    \"\"\"\n",
        "    Generate time-based features for forecasting.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame with 'timestamp' column.\n",
        "    \n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with added features.\n",
        "    \"\"\"\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['weekday'] = df['timestamp'].dt.weekday\n",
        "    df['IS_DAY'] = df['hour'].apply(lambda x: 1 if 8 <= x < 21 else 0)\n",
        "    df['IS_LUNCH'] = df['hour'].apply(lambda x: 0.2 if 12 <= x < 14 else 0)\n",
        "    df['IS_WORKTIME'] = df['hour'].apply(lambda x: 0.2 if 8 <= x < 19 else 0)\n",
        "    df['IS_MORNING'] = df['hour'].apply(lambda x: 0.4 if 8 <= x < 10 else 0)\n",
        "    df['WORKDAYS'] = df['weekday'].apply(lambda x: 1 if x < 5 else 0)\n",
        "    df['HOLIDAYS'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    df['minute'] = df['timestamp'].dt.minute\n",
        "    df['half_hour'] = df['hour'] * 2 + df['minute'] // 30\n",
        "\n",
        "    for half_hour in range(48):\n",
        "        hour = half_hour // 2\n",
        "        minute = (half_hour % 2) * 30\n",
        "        col_name = f'IS_0_d_{hour:02d}_{minute:02d}_00'\n",
        "        df[col_name] = df['half_hour'].apply(lambda x: 1 if x == half_hour else 0)\n",
        "\n",
        "    weekday_dummies = pd.get_dummies(df['weekday'], prefix='weekday').astype(int)\n",
        "    df = df.join(weekday_dummies, how='left')\n",
        "\n",
        "    for i in range(7):\n",
        "        col_name = f'weekday_{i}'\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = 0\n",
        "\n",
        "    df['day'] = df['timestamp'].dt.day\n",
        "    df['critical_day'] = df['day'].apply(is_critical_day)\n",
        "    df['billing_day'] = df['day'].apply(is_billing_day)\n",
        "\n",
        "    df = df.drop(['hour', 'minute', 'half_hour', 'weekday', 'day'], axis=1)\n",
        "    return df\n",
        "\n",
        "# Create future DataFrame\n",
        "future_df = pd.DataFrame(time_range, columns=['timestamp'])\n",
        "future_df = generate_features(future_df)\n",
        "\n",
        "# Add lag and rolling columns\n",
        "for i in range(1, 11):\n",
        "    future_df[f'Inq_{i}'] = np.nan\n",
        "future_df['Inq_std_3'] = np.nan\n",
        "future_df['Inq_mean_7'] = np.nan\n",
        "future_df['CNT_CALLS'] = np.nan\n",
        "\n",
        "future_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Reorder columns\n",
        "columns_order = ['HOLIDAYS', 'WORKDAYS', 'IS_DAY', 'IS_LUNCH', 'IS_WORKTIME', 'IS_MORNING',\n",
        "                 'CNT_CALLS', 'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3',\n",
        "                 'weekday_4', 'weekday_5', 'weekday_6',\n",
        "                 'IS_0_d_08_00_00', 'IS_0_d_08_30_00', 'IS_0_d_09_00_00',\n",
        "                 'IS_0_d_09_30_00', 'IS_0_d_10_00_00', 'IS_0_d_10_30_00',\n",
        "                 'IS_0_d_11_00_00', 'IS_0_d_11_30_00', 'IS_0_d_12_00_00',\n",
        "                 'IS_0_d_12_30_00', 'IS_0_d_13_00_00', 'IS_0_d_13_30_00',\n",
        "                 'IS_0_d_14_00_00', 'IS_0_d_14_30_00', 'IS_0_d_15_00_00',\n",
        "                 'IS_0_d_15_30_00', 'IS_0_d_16_00_00', 'IS_0_d_16_30_00',\n",
        "                 'IS_0_d_17_00_00', 'IS_0_d_17_30_00', 'IS_0_d_18_00_00',\n",
        "                 'IS_0_d_18_30_00', 'IS_0_d_19_00_00', 'IS_0_d_19_30_00',\n",
        "                 'IS_0_d_20_00_00', 'IS_0_d_20_30_00', 'IS_0_d_21_00_00',\n",
        "                 'IS_0_d_21_30_00', 'critical_day', 'billing_day', 'Inq_1', 'Inq_2', 'Inq_3', 'Inq_4', 'Inq_5', 'Inq_6',\n",
        "                 'Inq_7', 'Inq_8', 'Inq_9', 'Inq_10',\n",
        "                 'Inq_mean_7', 'Inq_std_3']\n",
        "future_df = future_df[columns_order]\n",
        "future_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine historical and future data\n",
        "combined_df = pd.concat([final_df, future_df], ignore_index=False)\n",
        "\n",
        "# Function to calculate lag and rolling features\n",
        "def calculate_inq_features(df, start_index, predict_index, end_index):\n",
        "    \"\"\"\n",
        "    Calculate lag features and rolling statistics for the DataFrame.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pd.DataFrame): Combined DataFrame.\n",
        "    start_index (pd.Timestamp): Start index for calculation.\n",
        "    predict_index (pd.Timestamp): Prediction start index.\n",
        "    end_index (pd.Timestamp): End index.\n",
        "    \n",
        "    Returns:\n",
        "    pd.DataFrame: Updated DataFrame with features.\n",
        "    \"\"\"\n",
        "    for i in range(1, 11):\n",
        "        df[f'Inq_{i}'] = df['CNT_CALLS'].shift(i)\n",
        "    df['Inq_mean_7'] = df['Inq_1'].rolling(window=7).mean()\n",
        "    df['Inq_std_3'] = df['Inq_1'].rolling(window=3).std()\n",
        "    return df.loc[start_index:end_index]\n",
        "\n",
        "# Determine indices\n",
        "selected_df = combined_df.loc[start_time]\n",
        "selected_index = combined_df.index.get_loc(selected_df.name)\n",
        "target_index = max(selected_index - 10, 0)\n",
        "previous_record_index = combined_df.index[target_index]\n",
        "start_index = previous_record_index\n",
        "\n",
        "# Calculate features\n",
        "calc_df = calculate_inq_features(combined_df, start_index, start_time, end_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions iteratively\n",
        "current_index = pd.to_datetime(start_time)\n",
        "end_index = pd.to_datetime(end_time)\n",
        "predicted_calls = []\n",
        "\n",
        "while current_index <= end_index:\n",
        "    calc_df.loc[:current_index, numerical_columns] = scaler_x.transform(calc_df.loc[:current_index, numerical_columns])\n",
        "    features = calc_df.loc[current_index, [\n",
        "        'HOLIDAYS', 'WORKDAYS', 'IS_DAY', 'IS_LUNCH', 'IS_WORKTIME', 'IS_MORNING',\n",
        "        'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3',\n",
        "        'weekday_4', 'weekday_5', 'weekday_6', 'IS_0_d_08_00_00',\n",
        "        'IS_0_d_08_30_00', 'IS_0_d_09_00_00', 'IS_0_d_09_30_00',\n",
        "        'IS_0_d_10_00_00', 'IS_0_d_10_30_00', 'IS_0_d_11_00_00',\n",
        "        'IS_0_d_11_30_00', 'IS_0_d_12_00_00', 'IS_0_d_12_30_00',\n",
        "        'IS_0_d_13_00_00', 'IS_0_d_13_30_00', 'IS_0_d_14_00_00',\n",
        "        'IS_0_d_14_30_00', 'IS_0_d_15_00_00', 'IS_0_d_15_30_00',\n",
        "        'IS_0_d_16_00_00', 'IS_0_d_16_30_00', 'IS_0_d_17_00_00',\n",
        "        'IS_0_d_17_30_00', 'IS_0_d_18_00_00', 'IS_0_d_18_30_00',\n",
        "        'IS_0_d_19_00_00', 'IS_0_d_19_30_00', 'IS_0_d_20_00_00',\n",
        "        'IS_0_d_20_30_00', 'IS_0_d_21_00_00', 'IS_0_d_21_30_00',\n",
        "        'critical_day', 'billing_day', 'Inq_1', 'Inq_2', 'Inq_3', 'Inq_4', 'Inq_5', 'Inq_6',\n",
        "        'Inq_7', 'Inq_8', 'Inq_9', 'Inq_10', 'Inq_mean_7', 'Inq_std_3'\n",
        "    ]]\n",
        "    features_array = np.array(features).reshape(1, 1, -1)\n",
        "    predicted_call = model.predict(features_array)\n",
        "    predicted_call_inverse = scaler_y.inverse_transform(predicted_call.reshape(-1, 1))\n",
        "    predicted_calls.append(predicted_call_inverse[0, 0])\n",
        "    calc_df.loc[current_index, 'CNT_CALLS'] = predicted_call_inverse\n",
        "    for i in range(1, 11):\n",
        "        calc_df.loc[:, f'Inq_{i}'] = calc_df['CNT_CALLS'].shift(i)\n",
        "    calc_df.loc[:, 'Inq_mean_7'] = calc_df['Inq_1'].rolling(window=7).mean()\n",
        "    calc_df.loc[:, 'Inq_std_3'] = calc_df['Inq_1'].rolling(window=3).std()\n",
        "    current_index += pd.Timedelta(minutes=30)\n",
        "\n",
        "print(predicted_calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter and round predictions\n",
        "calc_df.index = pd.to_datetime(calc_df.index)\n",
        "filtered_df = calc_df.between_time('08:00', '21:30')\n",
        "filtered_df['CNT_CALLS'] = filtered_df['CNT_CALLS'].round()\n",
        "filtered_df.to_excel('calc_df LSTM октябрь.xlsx', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(20, 6))\n",
        "plt.plot(calc_df[['CNT_CALLS']].sort_index(), label=['Predictions'])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with historical data\n",
        "plt.figure(figsize=(20, 6))\n",
        "plt.plot(calc_df[['CNT_CALLS']].sort_index(), label='Predictions')\n",
        "plt.plot(final_df.index, final_df['CNT_CALLS'], label='Historical Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operator Forecasting Using ErlangC\n",
        "\n",
        "Calculate required operators based on predicted call volumes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load predicted data\n",
        "calc_df = pd.read_csv('calc_df 092024.csv', sep=';', header=0, parse_dates=True)\n",
        "\n",
        "# Create timestamp\n",
        "calc_df['MONTH'] = pd.to_datetime(calc_df['MONTH'] + '-01')\n",
        "calc_df['DATESTART'] = pd.to_datetime(calc_df['DATESTART'], dayfirst=True)\n",
        "calc_df['RAZREZ'] = pd.to_timedelta(calc_df['RAZREZ'] + ':00')\n",
        "calc_df['TIMESTAMP'] = calc_df['DATESTART'] + calc_df['RAZREZ']\n",
        "calc_df.set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "# Prepare DataFrame for operators\n",
        "for_oper = calc_df[['CNT_CALLS']].copy()\n",
        "for_oper['CNT_CALLS'] = for_oper['CNT_CALLS'].str.replace(',', '.').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyworkforce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyworkforce.queuing import ErlangC\n",
        "\n",
        "# Fixed parameters for ErlangC\n",
        "asa = 20 / 60\n",
        "aht = 3.0\n",
        "interval = 30\n",
        "shrinkage = 0.29\n",
        "service_level = 0.8\n",
        "max_occupancy = 0.75\n",
        "\n",
        "def calculate_required_positions(transactions):\n",
        "    \"\"\"\n",
        "    Calculate required positions using ErlangC model.\n",
        "    \n",
        "    Parameters:\n",
        "    transactions (float): Number of transactions (calls).\n",
        "    \n",
        "    Returns:\n",
        "    dict: Dictionary with raw_positions, positions, service_level, occupancy, waiting_probability.\n",
        "    \"\"\"\n",
        "    erlang = ErlangC(transactions=transactions, asa=asa, aht=aht, interval=interval, shrinkage=shrinkage)\n",
        "    return erlang.required_positions(service_level=service_level, max_occupancy=max_occupancy)\n",
        "\n",
        "# Apply to DataFrame\n",
        "for_oper[['raw_positions', 'positions', 'service_level', 'occupancy', 'waiting_probability']] = for_oper['CNT_CALLS'].apply(\n",
        "    lambda x: pd.Series(calculate_required_positions(x))\n",
        ")\n",
        "\n",
        "for_oper.head(30)\n",
        "for_oper.to_excel('for_oper октябрь.xlsx', index=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}