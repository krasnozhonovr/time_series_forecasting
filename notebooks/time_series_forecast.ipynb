{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìû Call Center Forecasting ‚Äî end-to-end notebook\n",
        "\n",
        "Description: Demonstrates full pipeline: load -> anomaly detection -> feature engineering -> train (RandomForest & LSTM) -> evaluate -> forecast -> save results."
      ],
      "metadata": {
        "id": "pFy4NdF3Ifaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 0. Imports and global settings ===\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 5)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "import joblib   # for saving models\n",
        "import json\n",
        "\n",
        "# Set project paths (adjust if needed)\n",
        "ROOT = Path(\"..\")  # if notebook is in notebooks/, project root is one level up\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "RESULTS_DIR = ROOT / \"results\"\n",
        "SRC_DIR = ROOT / \"src\"\n",
        "\n",
        "# Ensure results directory exists\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "KWMBuSvLIgSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Utility functions\n",
        "–ó–¥–µ—Å—å –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: –∑–∞–≥—Ä—É–∑–∫–∞, –±–∞–∑–æ–≤–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è, –æ—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫.\n"
      ],
      "metadata": {
        "id": "FVtVaz6aInqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Utilities ===\n",
        "def load_csv(path: Path, sep: str=';', parse_dates: bool=True) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV with expected separators; returns DataFrame.\"\"\"\n",
        "    return pd.read_csv(path, sep=sep, header=0)\n",
        "\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    \"\"\"Return MAE, RMSE, R2 in a dict.\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"R2\": float(r2)}\n",
        "\n",
        "def plot_series(ts, label='value', title=None):\n",
        "    \"\"\"Simple plotting helper.\"\"\"\n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.plot(ts, label=label)\n",
        "    plt.title(title or label)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(label)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def save_json(obj, path: Path):\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(obj, f, indent=2)\n"
      ],
      "metadata": {
        "id": "nDCitDhKIpWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load & initial preprocessing\n",
        "–ó–∞–≥—Ä—É–∂–∞–µ–º CSV, –ø–∞—Ä—Å–∏–º –¥–∞—Ç—ã, —Å–æ–∑–¥–∞—ë–º TIMESTAMP, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã.\n",
        "(–ê–¥–∞–ø—Ç–∏—Ä—É–π –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –ø–æ–¥ —Å–≤–æ–π —Ñ–∞–π–ª: `DATESTART`, `RAZREZ`, `MONTH`, `CNT_CALLS`.)\n"
      ],
      "metadata": {
        "id": "uglB-JFiIrEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. Load and initial preprocessing ===\n",
        "csv_path = DATA_DIR / \"calls.csv\"   # <- –∑–∞–º–µ–Ω–∏—Ç–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "if not csv_path.exists():\n",
        "    rng = pd.date_range(start=\"2024-01-01\", periods=24*30, freq=\"H\")\n",
        "    df_synth = pd.DataFrame({\n",
        "        \"DATESTART\": rng.date,\n",
        "        \"RAZREZ\": rng.time.astype(str).str[:5],  # 'HH:MM'\n",
        "        \"CNT_CALLS\": (200 + 50*np.sin(np.arange(len(rng))/24) + np.random.poisson(10, len(rng))).astype(int),\n",
        "        \"MONTH\": rng.month.astype(str)\n",
        "    })\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    df_synth.to_csv(csv_path, sep=';', index=False)\n",
        "    print(\"Synthetic dataset created:\", csv_path)\n",
        "\n",
        "raw = load_csv(csv_path, sep=';')\n",
        "raw.head(3)"
      ],
      "metadata": {
        "id": "YWOgFWkcIslR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize and parse date/time columns - robust approach with checks\n",
        "def prepare_timestamp(df: pd.DataFrame,\n",
        "                      date_col='DATESTART',\n",
        "                      time_col='RAZREZ',\n",
        "                      month_col='MONTH') -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # If time column contains hh:mm or hh:mm:ss, normalize to HH:MM:SS\n",
        "    if df[time_col].dtype == object:\n",
        "        df[time_col] = df[time_col].astype(str).str.strip()\n",
        "        # ensure seconds part\n",
        "        df[time_col] = df[time_col].apply(lambda x: x if len(x.split(':'))==3 else (x + ':00'))\n",
        "    # Parse date and time\n",
        "    df[date_col] = pd.to_datetime(df[date_col], dayfirst=True, errors='coerce')\n",
        "    df[time_col] = pd.to_timedelta(df[time_col])\n",
        "    df['TIMESTAMP'] = df[date_col] + df[time_col]\n",
        "    # Optional: month -> first day of month\n",
        "    if month_col in df.columns:\n",
        "        try:\n",
        "            df[month_col] = pd.to_datetime(df[month_col].astype(str) + '-01', errors='coerce')\n",
        "        except Exception:\n",
        "            pass\n",
        "    return df\n",
        "\n",
        "df = prepare_timestamp(raw)\n",
        "df = df.sort_values('TIMESTAMP').reset_index(drop=True)\n",
        "plot_series(df['CNT_CALLS'], label='Raw calls', title='Raw call counts (unsmoothed)')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "tqbtlKjUI20t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Anomaly detection & correction\n",
        "–ò—Å–ø–æ–ª—å–∑—É–µ–º IsolationForest (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥). –ü–æ—Å–ª–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ ‚Äî –∑–∞–º–µ–Ω—è–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ –º–µ–¥–∏–∞–Ω–æ–π –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n"
      ],
      "metadata": {
        "id": "k4ZQEcvRIzlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3. Anomaly detection and correction ===\n",
        "def detect_and_fix_anomalies(df: pd.DataFrame,\n",
        "                             value_col='CNT_CALLS',\n",
        "                             contamination=0.001,\n",
        "                             random_state=42):\n",
        "    df = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    vals = df[[value_col]].fillna(0).values\n",
        "    vals_scaled = scaler.fit_transform(vals)\n",
        "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
        "    labels = iso.fit_predict(vals_scaled)\n",
        "    df['anomaly_flag'] = (labels == -1)\n",
        "    n_anom = df['anomaly_flag'].sum()\n",
        "    if n_anom > 0:\n",
        "        median_val = df.loc[~df['anomaly_flag'], value_col].median()\n",
        "        df.loc[df['anomaly_flag'], value_col] = median_val\n",
        "    else:\n",
        "        median_val = None\n",
        "    df.drop(columns=['anomaly_flag'], inplace=True)\n",
        "    return df, n_anom, median_val\n",
        "\n",
        "df_clean, n_anom, med = detect_and_fix_anomalies(df, value_col='CNT_CALLS', contamination=0.001)\n",
        "print(f\"Detected anomalies: {n_anom}; replaced with median={med}\")\n",
        "plot_series(df['CNT_CALLS'], label='Original')\n",
        "plot_series(df_clean['CNT_CALLS'], label='Cleaned')\n"
      ],
      "metadata": {
        "id": "OwV49Z08I7dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature engineering\n",
        "–°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –ª–∞–≥–∏, —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ, billing/critical days, one-hot –¥–ª—è weekday –∏ –ø–æ–ª—É-—á–∞—Å–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã.\n"
      ],
      "metadata": {
        "id": "KVUqZSh7I9hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4. Feature engineering ===\n",
        "def create_time_features(df: pd.DataFrame, timestamp_col='TIMESTAMP', value_col='CNT_CALLS', max_lag=10):\n",
        "    df = df.copy()\n",
        "    df = df.set_index(timestamp_col).asfreq('30T')  # ensure uniform frequency (30 minutes)\n",
        "    # Fill missing CNT_CALLS with interpolation or 0 (choose strategy)\n",
        "    df[value_col] = df[value_col].interpolate().fillna(method='bfill').astype(float)\n",
        "\n",
        "    # Basic time features\n",
        "    df['hour'] = df.index.hour\n",
        "    df['minute'] = df.index.minute\n",
        "    df['weekday'] = df.index.weekday  # Monday=0\n",
        "    df['is_day'] = ((df['hour'] >= 8) & (df['hour'] < 21)).astype(int)\n",
        "    df['is_lunch'] = df['hour'].between(12,13).astype(int) * 0.2\n",
        "    df['is_worktime'] = df['hour'].between(8,18).astype(int) * 0.2\n",
        "    df['is_morning'] = df['hour'].between(8,9).astype(int) * 0.4\n",
        "    df['workdays'] = (df['weekday'] < 5).astype(int)\n",
        "    df['holidays'] = (df['weekday'] >=5).astype(int)\n",
        "    df['day'] = df.index.day\n",
        "\n",
        "    # billing & critical day flags (adapt lists if needed)\n",
        "    df['billing_day'] = df['day'].isin([5,15,25]).astype(int)\n",
        "    df['critical_day'] = df['day'].isin([3,4,13,14,23,24]).astype(int)\n",
        "\n",
        "    # one-hot weekdays\n",
        "    weekdays = pd.get_dummies(df['weekday'], prefix='weekday')\n",
        "    df = pd.concat([df, weekdays], axis=1)\n",
        "\n",
        "    # half-hour indicators (IS_0_d_HH_MM_00 style)\n",
        "    half_hour_index = (df.index.hour * 2 + (df.index.minute // 30)).astype(int)\n",
        "    for hh in range(48):\n",
        "        df[f'IS_half_{hh:02d}'] = (half_hour_index == hh).astype(int)\n",
        "\n",
        "    # lag features\n",
        "    for lag in range(1, max_lag+1):\n",
        "        df[f'lag_{lag}'] = df[value_col].shift(lag)\n",
        "\n",
        "    # rolling features\n",
        "    df['rolling_mean_7'] = df[value_col].shift(1).rolling(window=7).mean()\n",
        "    df['rolling_std_3'] = df[value_col].shift(1).rolling(window=3).std()\n",
        "\n",
        "    # drop rows with NaNs introduced by lags\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "feature_df = create_time_features(df_clean, timestamp_col='TIMESTAMP', value_col='CNT_CALLS', max_lag=10)\n",
        "feature_df.shape\n"
      ],
      "metadata": {
        "id": "pw9aydL5I92x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick peek and sanity plots\n",
        "feature_df.head(3)\n",
        "plot_series(feature_df['CNT_CALLS'], label='CNT_CALLS (processed)', title='Processed Call Volume')"
      ],
      "metadata": {
        "id": "-wLMqWn0JBDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train/Test split and scaling\n",
        "–î–µ–ª–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–µ–∑, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ —Ñ–∏—á–∏. –ò—Å–ø–æ–ª—å–∑—É–µ–º shuffle=False –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤."
      ],
      "metadata": {
        "id": "-RPxN5a-I_FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5. Train/Test split and scaling ===\n",
        "target = 'CNT_CALLS'\n",
        "features = [c for c in feature_df.columns if c != target]\n",
        "\n",
        "# Time-based train/test split (70%/30% or custom)\n",
        "train_size = int(len(feature_df) * 0.7)\n",
        "train_df = feature_df.iloc[:train_size]\n",
        "test_df = feature_df.iloc[train_size:]\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target]\n",
        "X_test = test_df[features]\n",
        "y_test = test_df[target]\n",
        "\n",
        "# Scale numeric columns only (lags and rolling)\n",
        "numeric_cols = [c for c in X_train.columns if c.startswith('lag_') or 'rolling' in c]\n",
        "scaler_x = StandardScaler()\n",
        "X_train[numeric_cols] = scaler_x.fit_transform(X_train[numeric_cols])\n",
        "X_test[numeric_cols] = scaler_x.transform(X_test[numeric_cols])\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1,1))\n",
        "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1))\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "bfNwrVgtJEBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. RandomForest baseline model\n",
        "–¢—Ä–µ–Ω–∏—Ä—É–µ–º RandomForest —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ / tuned –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏; –æ—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ —Ç–µ—Å—Ç–µ."
      ],
      "metadata": {
        "id": "_MJiGIHtJGfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6. RandomForest baseline ===\n",
        "rf = RandomForestRegressor(n_estimators=1000, max_depth=None, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "metrics_rf = evaluate_regression(y_test, y_pred_rf)\n",
        "print(\"RandomForest metrics:\", metrics_rf)\n",
        "plot_series(y_test.values, label='Actual', title='RandomForest ‚Äî Actual vs Pred')\n",
        "plt.plot(y_pred_rf, label='RF Predictions', alpha=0.8)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save RF model\n",
        "joblib.dump(rf, RESULTS_DIR / \"rf_model.joblib\")\n",
        "save_json(metrics_rf, RESULTS_DIR / \"metrics_rf.json\")\n"
      ],
      "metadata": {
        "id": "B0LbWDAWJIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. LSTM model (simple 1-step LSTM)\n",
        "–ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ (samples, timesteps, features). –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º time_steps=1, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏.\n"
      ],
      "metadata": {
        "id": "JtEwDjXwJNKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7. LSTM model ===\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# reshape inputs for LSTM: (samples, timesteps, features)\n",
        "time_steps = 1\n",
        "X_train_lstm = X_train.values.reshape((X_train.shape[0], time_steps, X_train.shape[1]))\n",
        "X_test_lstm = X_test.values.reshape((X_test.shape[0], time_steps, X_test.shape[1]))\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_lstm, y_train_scaled, validation_data=(X_test_lstm, y_test_scaled),\n",
        "                    epochs=50, batch_size=32, callbacks=[es], verbose=1)\n",
        "\n",
        "# Predictions and inverse scaling\n",
        "y_pred_lstm_scaled = model.predict(X_test_lstm)\n",
        "y_pred_lstm = scaler_y.inverse_transform(y_pred_lstm_scaled).ravel()\n",
        "\n",
        "metrics_lstm = evaluate_regression(y_test, y_pred_lstm)\n",
        "print(\"LSTM metrics:\", metrics_lstm)\n",
        "\n",
        "# Save LSTM model\n",
        "model.save(RESULTS_DIR / \"lstm_model.h5\")\n",
        "save_json(metrics_lstm, RESULTS_DIR / \"metrics_lstm.json\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.title(\"LSTM training loss\")\n",
        "plt.show()\n",
        "\n",
        "plot_series(y_test.values, label='Actual')\n",
        "plt.plot(y_pred_lstm, label='LSTM Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fEwlkjKQJNdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Forecasting (rolling predictions for future timestamps)\n",
        "–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±—É–¥—É—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ—à–∞–≥–æ–≤–æ (auto-regressive style): –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–æ–≤–æ–≥–æ lag.\n"
      ],
      "metadata": {
        "id": "YfpY3G_zJPpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 8. Forecasting future periods (auto-regressive loop) ===\n",
        "from datetime import timedelta\n",
        "\n",
        "def generate_future_index(last_timestamp, periods=48, freq='30T'):\n",
        "    return pd.date_range(start=last_timestamp + pd.Timedelta(freq), periods=periods, freq=freq)\n",
        "\n",
        "def prepare_future_df(feature_df, last_timestamp, periods=48, freq='30T'):\n",
        "    future_idx = generate_future_index(last_timestamp, periods=periods, freq=freq)\n",
        "    future_df = pd.DataFrame(index=future_idx)\n",
        "    # create time features using same logic as create_time_features but on empty df\n",
        "    future_df['hour'] = future_df.index.hour\n",
        "    future_df['minute'] = future_df.index.minute\n",
        "    future_df['weekday'] = future_df.index.weekday\n",
        "    future_df['is_day'] = ((future_df['hour'] >= 8) & (future_df['hour'] < 21)).astype(int)\n",
        "    future_df['is_lunch'] = future_df['hour'].between(12,13).astype(int) * 0.2\n",
        "    future_df['is_worktime'] = future_df['hour'].between(8,18).astype(int) * 0.2\n",
        "    future_df['is_morning'] = future_df['hour'].between(8,9).astype(int) * 0.4\n",
        "    future_df['workdays'] = (future_df['weekday'] < 5).astype(int)\n",
        "    future_df['holidays'] = (future_df['weekday'] >= 5).astype(int)\n",
        "    future_df['day'] = future_df.index.day\n",
        "    future_df['billing_day'] = future_df['day'].isin([5,15,25]).astype(int)\n",
        "    future_df['critical_day'] = future_df['day'].isin([3,4,13,14,23,24]).astype(int)\n",
        "    # weekdays one-hot\n",
        "    weekdays = pd.get_dummies(future_df['weekday'], prefix='weekday')\n",
        "    future_df = pd.concat([future_df, weekdays], axis=1)\n",
        "    # half-hour indicators\n",
        "    half_hour_index = (future_df.index.hour * 2 + (future_df.index.minute // 30)).astype(int)\n",
        "    for hh in range(48):\n",
        "        future_df[f'IS_half_{hh:02d}'] = (half_hour_index == hh).astype(int)\n",
        "    # initialize lag and rolling features as NaN (will be filled iteratively)\n",
        "    for lag in range(1, 11):\n",
        "        future_df[f'lag_{lag}'] = np.nan\n",
        "    future_df['rolling_mean_7'] = np.nan\n",
        "    future_df['rolling_std_3'] = np.nan\n",
        "    return future_df\n",
        "\n",
        "# Prepare combined df with history + future skeleton\n",
        "last_time = feature_df.index[-1]\n",
        "horizon = 48*2  # example: 48 half-hours -> 24 hours; change as needed\n",
        "future_skeleton = prepare_future_df(feature_df, last_time, periods=horizon)\n",
        "combined = pd.concat([feature_df, future_skeleton], axis=0)\n",
        "\n",
        "# Fill initial lags for future rows from historic values\n",
        "for i in range(1, 11):\n",
        "    combined[f'lag_{i}'] = combined['CNT_CALLS'].shift(i)\n",
        "\n",
        "# Iterative forecasting using LSTM model (you can switch to RF similarly)\n",
        "predictions = []\n",
        "model_used = 'lstm'  # or 'rf' if you prefer\n",
        "for t in future_skeleton.index:\n",
        "    row = combined.loc[t]\n",
        "    # If any numeric features require scaling, do it like below\n",
        "    features_row = combined.loc[t, features].copy()\n",
        "    # For numeric columns (lags + rolling), ensure they are imputed (use last known)\n",
        "    for col in features_row.index:\n",
        "        if pd.isna(features_row[col]):\n",
        "            # fallback: take last available value for that column\n",
        "            features_row[col] = combined[col].ffill().iloc[-1]\n",
        "    # Scale numeric cols\n",
        "    features_row[numeric_cols] = scaler_x.transform(features_row[numeric_cols].values.reshape(1, -1))\n",
        "    # reshape for LSTM\n",
        "    X_row = features_row.values.reshape(1, time_steps, -1)\n",
        "    pred_scaled = model.predict(X_row)\n",
        "    pred = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1))[0,0]\n",
        "    # write prediction into combined\n",
        "    combined.loc[t, 'CNT_CALLS'] = pred\n",
        "    # update subsequent lag columns for future rows\n",
        "    # shift lags (this is a simple approach; for production, implement more robust rolling)\n",
        "    for lag in range(10, 1, -1):\n",
        "        combined.loc[t:, f'lag_{lag}'] = combined.loc[t:, f'lag_{lag-1}']\n",
        "    combined.loc[t:, 'lag_1'] = combined['CNT_CALLS'].shift(1).loc[t:]\n",
        "    # recompute rolling features for rows after current t\n",
        "    combined.loc[:, 'rolling_mean_7'] = combined['CNT_CALLS'].shift(1).rolling(window=7).mean()\n",
        "    combined.loc[:, 'rolling_std_3'] = combined['CNT_CALLS'].shift(1).rolling(window=3).std()\n",
        "    predictions.append((t, pred))\n",
        "\n",
        "# Collect forecasted series\n",
        "forecast_df = pd.DataFrame(predictions, columns=['timestamp','forecast_calls']).set_index('timestamp')\n",
        "forecast_df['forecast_calls'] = forecast_df['forecast_calls'].round().astype(int)\n",
        "forecast_df.head(20)\n",
        "\n",
        "# Save forecast\n",
        "forecast_df.to_csv(RESULTS_DIR / \"forecast_output.csv\")\n",
        "plot_series(forecast_df['forecast_calls'], label='Forecast (auto-regressive)', title='Future Forecast')\n"
      ],
      "metadata": {
        "id": "JsameMoJJQx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Operator staffing calculation (Erlang-C)\n",
        "–ü—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É `pyworkforce` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).\n"
      ],
      "metadata": {
        "id": "U49rHW1VJSbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 9. Staffing calculation (optional) ===\n",
        "try:\n",
        "    from pyworkforce.queuing import ErlangC\n",
        "    print(\"pyworkforce available ‚Äî computing required operators for first forecast window\")\n",
        "    sample_calls = int(forecast_df['forecast_calls'].iloc[:1].values[0])\n",
        "    erlang = ErlangC(transactions=sample_calls, asa=20/60, aht=3.14, interval=60, shrinkage=0.29)\n",
        "    req = erlang.required_positions(service_level=0.8, max_occupancy=0.75)\n",
        "    print(\"Example required operators (sample):\", req)\n",
        "except Exception as e:\n",
        "    print(\"pyworkforce not installed or error occurred. Skip staffing calc. Error:\", e)\n"
      ],
      "metadata": {
        "id": "LCeBYqDtJcu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Save models, metrics and wrap-up\n",
        "–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ –∏ –æ—Ç—á—ë—Ç—ã –≤ /results.\n"
      ],
      "metadata": {
        "id": "LBZ6EldKJfb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 10. Save models & metrics (already saved earlier for RF & LSTM) ===\n",
        "# Save also final forecast plot\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(feature_df['CNT_CALLS'].iloc[-48*7:], label='Recent actual')\n",
        "plt.plot(forecast_df['forecast_calls'].iloc[:48*3], label='Forecast (first 3 days)')\n",
        "plt.legend()\n",
        "plt.title(\"Recent actual vs forecast\")\n",
        "plt.savefig(RESULTS_DIR / \"forecast_plot.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save combined metrics summary\n",
        "all_metrics = {\"rf\": metrics_rf, \"lstm\": metrics_lstm}\n",
        "save_json(all_metrics, RESULTS_DIR / \"summary_metrics.json\")\n",
        "print(\"Saved metrics and forecast in:\", RESULTS_DIR.resolve())\n"
      ],
      "metadata": {
        "id": "AdCBLONfJhVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Done\n",
        "Notebook demonstrates end-to-end approach: data cleaning, anomaly detection, feature engineering, model training, evaluation, and forecasting.\n"
      ],
      "metadata": {
        "id": "qVY5ddD4Ji3V"
      }
    }
  ]
}